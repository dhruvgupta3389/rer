import { type FlashcardData } from '../components/Flashcard';

export const dlUnit4Data: FlashcardData[] = [
    {
        id: 1,
        title: "1. Optimization in Deep Learning",
        frontText: "Challenges of Training Deep Networks",
        backTitle: "Optimization Challenges",
        importance: "high",
        backContent: [
            "ğŸ¯ GOAL: Find parameters Î¸ that minimize loss L(Î¸)",
            "âš ï¸ CHALLENGES:",
            "  â€¢ Non-convex loss surface (many local minima)",
            "  â€¢ Saddle points (more common than local minima in high-D)",
            "  â€¢ Ill-conditioning (different curvature in different directions)",
            "  â€¢ Vanishing/Exploding gradients",
            "ğŸ”§ SOLUTIONS:",
            "  â€¢ Momentum: Accelerate through flat regions",
            "  â€¢ Adaptive learning rates: Adam, RMSprop",
            "  â€¢ Batch normalization: Smooth loss landscape",
            "  â€¢ Skip connections: Better gradient flow",
            "âš¡ AKTU: Explain non-convex optimization challenges"
        ]
    },
    {
        id: 2,
        title: "2. Non-Convex Optimization",
        frontText: "Why Deep Learning Still Works",
        backTitle: "Non-Convex Properties",
        importance: "medium",
        backContent: [
            "ğŸ“Š CONVEX vs NON-CONVEX:",
            "  â€¢ Convex: Single global minimum, easy optimization",
            "  â€¢ Non-convex: Multiple local minima, saddle points",
            "ğŸ§  SURPRISING FINDINGS:",
            "  â€¢ Most local minima are nearly as good as global",
            "  â€¢ Saddle points are bigger problem than local minima",
            "  â€¢ Flat minima â†’ Better generalization (Hochreiter)",
            "ğŸ“ˆ LOSS LANDSCAPE VISUALIZATION:",
            "  â€¢ Sharp minima: Poor generalization",
            "  â€¢ Flat/wide minima: Good generalization",
            "ğŸ”§ TECHNIQUES: Large batch â†’ Sharp | Small batch â†’ Flat",
            "âš¡ AKTU: Compare convex vs non-convex, saddle point problem"
        ]
    },
    {
        id: 3,
        title: "3. Stochastic Optimization",
        frontText: "Practical Optimization Algorithms",
        backTitle: "Optimizer Comparison",
        importance: "high",
        backContent: [
            "âš¡ SGD with MOMENTUM:",
            "  v = Î²Â·v + Î·Â·âˆ‡L, Î¸ = Î¸ - v",
            "  (Î² typically 0.9, like ball rolling downhill)",
            "ğŸ“Š ADAGRAD: Adaptive per-parameter learning rates",
            "  Î¸ = Î¸ - Î·/âˆš(Î£gâ‚œÂ²) Â· g (slower for frequent features)",
            "ğŸ”§ RMSPROP: Fix AdaGrad's shrinking LR",
            "  E[gÂ²]â‚œ = ÏÂ·E[gÂ²]â‚œâ‚‹â‚ + (1-Ï)gâ‚œÂ²",
            "ğŸŒŸ ADAM (Best of both):",
            "  m = Î²â‚m + (1-Î²â‚)g (momentum)",
            "  v = Î²â‚‚v + (1-Î²â‚‚)gÂ² (adaptive LR)",
            "  Î¸ = Î¸ - Î·Â·mÌ‚/âˆš(vÌ‚ + Îµ)",
            "âš¡ AKTU IMPORTANT: Adam algorithm, compare SGD vs Adam"
        ]
    },
    {
        id: 4,
        title: "4. Generalization in Neural Networks",
        frontText: "Why Do Overparameterized NNs Generalize?",
        backTitle: "Generalization Theory",
        importance: "medium",
        backContent: [
            "ğŸ¤” PUZZLE: NNs have more parameters than data, yet generalize!",
            "ğŸ“Š CLASSICAL VIEW: More params â†’ More overfitting",
            "ğŸ§  MODERN UNDERSTANDING:",
            "  â€¢ Implicit regularization from SGD",
            "  â€¢ Flat minima have better generalization",
            "  â€¢ Double descent: Error goes up then down with model size",
            "ğŸ“ˆ BIAS-VARIANCE TRADEOFF:",
            "  â€¢ High bias (underfit): Simple model, high training error",
            "  â€¢ High variance (overfit): Complex model, low train, high test",
            "ğŸ”§ REGULARIZATION HELPS: Dropout, weight decay, early stopping",
            "âš¡ AKTU: Bias-variance tradeoff, why deep networks generalize"
        ]
    },
    {
        id: 5,
        title: "5. Spatial Transformer Networks",
        frontText: "Learning Spatial Transformations",
        backTitle: "STN Architecture",
        importance: "medium",
        backContent: [
            "ğŸ¯ PROBLEM: CNNs not invariant to rotation, scale, distortion",
            "ğŸ’¡ SOLUTION: Learn to spatially transform input/features",
            "ğŸ—ï¸ THREE COMPONENTS:",
            "  1. LOCALIZATION NET: Predicts transformation params Î¸",
            "  2. GRID GENERATOR: Creates sampling grid from Î¸",
            "  3. SAMPLER: Bilinear interpolation on input",
            "ğŸ“ TRANSFORMATIONS:",
            "  â€¢ Affine: rotation, translation, scale, shear",
            "  â€¢ Î¸ = [a, b, tâ‚“; c, d, táµ§] (6 parameters)",
            "âœ… BENEFITS:",
            "  â€¢ End-to-end trainable",
            "  â€¢ Can be inserted anywhere in network",
            "  â€¢ Learns task-specific transformations",
            "âš¡ AKTU: STN components, affine transformation matrix"
        ]
    },
    {
        id: 6,
        title: "6. Recurrent Neural Networks (RNNs)",
        frontText: "Networks with Memory for Sequences",
        backTitle: "RNN Details",
        importance: "high",
        backContent: [
            "ğŸ¯ DESIGNED FOR: Sequential data (text, time series, audio)",
            "ğŸ”„ KEY IDEA: Hidden state hâ‚œ carries information from past",
            "ğŸ“ EQUATIONS:",
            "  hâ‚œ = tanh(Wâ‚•â‚•Â·hâ‚œâ‚‹â‚ + Wâ‚“â‚•Â·xâ‚œ + b)",
            "  yâ‚œ = Wâ‚•áµ§Â·hâ‚œ",
            "ğŸ“Š TYPES:",
            "  â€¢ One-to-One: Standard NN",
            "  â€¢ One-to-Many: Image â†’ Caption",
            "  â€¢ Many-to-One: Sentiment analysis",
            "  â€¢ Many-to-Many: Translation, Video classification",
            "âš ï¸ PROBLEM: Vanishing/Exploding gradients for long sequences",
            "âš¡ AKTU VERY IMPORTANT: RNN architecture, BPTT, gradient problems"
        ]
    },
    {
        id: 7,
        title: "7. Long Short-Term Memory (LSTM)",
        frontText: "Solving the Long-Term Dependency Problem",
        backTitle: "LSTM Gates",
        importance: "high",
        backContent: [
            "ğŸ¯ SOLVES: Vanishing gradient, learns long-term dependencies",
            "ğŸšª THREE GATES (all sigmoid â†’ [0,1]):",
            "  â€¢ FORGET Gate: fâ‚œ = Ïƒ(WfÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bf) â†’ What to forget",
            "  â€¢ INPUT Gate: iâ‚œ = Ïƒ(WiÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bi) â†’ What to store",
            "  â€¢ OUTPUT Gate: oâ‚œ = Ïƒ(WoÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bo) â†’ What to output",
            "ğŸ“¦ CELL STATE (memory highway):",
            "  CÌƒâ‚œ = tanh(WcÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bc) (candidate)",
            "  Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ (update cell)",
            "  hâ‚œ = oâ‚œ âŠ™ tanh(Câ‚œ) (hidden state)",
            "ğŸ”§ GRU: Simpler variant with 2 gates (reset, update)",
            "âš¡ AKTU VERY IMPORTANT: LSTM gate equations, diagram"
        ]
    },
    {
        id: 8,
        title: "8. Language Models & Word2Vec",
        frontText: "Word Embeddings and Language Modeling",
        backTitle: "Word Representations",
        importance: "high",
        backContent: [
            "ğŸ”¤ ONE-HOT: Sparse, no semantic meaning, huge vectors",
            "ğŸ§  WORD2VEC (Mikolov 2013): Dense embeddings (50-300 dims)",
            "ğŸ“Š TWO ARCHITECTURES:",
            "  â€¢ CBOW: Predict center word from context",
            "  â€¢ Skip-gram: Predict context from center word",
            "âœ¨ PROPERTIES:",
            "  â€¢ king - man + woman â‰ˆ queen",
            "  â€¢ Similar words have similar vectors",
            "ğŸ“ TRAINING: Maximize P(context|word) or P(word|context)",
            "ğŸ”§ TRICKS: Negative sampling, Hierarchical softmax",
            "ğŸ“ˆ MODERN: GloVe, FastText, BERT, GPT embeddings",
            "âš¡ AKTU Important: CBOW vs Skip-gram, word analogy task"
        ]
    },
    {
        id: 9,
        title: "9. Deep Reinforcement Learning",
        frontText: "Learning from Rewards",
        backTitle: "Deep RL Methods",
        importance: "high",
        backContent: [
            "ğŸ® RL BASICS: Agent, Environment, State, Action, Reward",
            "ğŸ¯ GOAL: Maximize cumulative reward Î£Î³áµ—râ‚œ",
            "ğŸ“Š KEY CONCEPTS:",
            "  â€¢ Policy Ï€(a|s): Probability of action given state",
            "  â€¢ Value V(s): Expected return from state s",
            "  â€¢ Q-value Q(s,a): Expected return from action a in state s",
            "ğŸ”· DQN (Deep Q-Network):",
            "  â€¢ NN approximates Q(s,a) â†’ Îµ-greedy action selection",
            "  â€¢ Experience replay + Target network for stability",
            "ğŸ”¶ POLICY GRADIENT:",
            "  â€¢ Directly optimize policy, REINFORCE algorithm",
            "ğŸ”´ ACTOR-CRITIC: Combine value and policy learning",
            "âš¡ AKTU: Q-learning, DQN architecture, policy gradient"
        ]
    },
    {
        id: 10,
        title: "10. Computational Neuroscience Connection",
        frontText: "Brain-Inspired Computing",
        backTitle: "Neuroscience & AI",
        importance: "normal",
        backContent: [
            "ğŸ§  BIOLOGICAL NEURON:",
            "  â€¢ Dendrites (input) â†’ Cell body â†’ Axon (output)",
            "  â€¢ Synapse: Connection strength (like weights)",
            "ğŸ”¬ INSPIRATIONS:",
            "  â€¢ Perceptron â† Biological neuron model",
            "  â€¢ CNN â† Visual cortex (Hubel & Wiesel)",
            "  â€¢ Attention â† Selective attention in brain",
            "  â€¢ Memory networks â† Hippocampus",
            "âš ï¸ DIFFERENCES:",
            "  â€¢ Biological: Analog, sparse, temporal, 3D",
            "  â€¢ Artificial: Digital, dense, static, layers",
            "ğŸ“Š MODERN RESEARCH: Spiking NNs, Neuromorphic chips",
            "âš¡ AKTU: Compare biological vs artificial neurons"
        ]
    }
];
