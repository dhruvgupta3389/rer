import { type FlashcardData } from '../components/Flashcard';

export const dlUnit2Data: FlashcardData[] = [
    {
        id: 1,
        title: "1. History of Deep Learning",
        frontText: "Evolution from Perceptron to Modern Deep Learning",
        backTitle: "Historical Timeline",
        importance: "medium",
        backContent: [
            "ğŸ“… 1943: McCulloch-Pitts neuron (mathematical model)",
            "ğŸ“… 1958: Perceptron (Rosenblatt) - First learning machine",
            "ğŸ“… 1969: Minsky & Papert - XOR problem â†’ AI Winter",
            "ğŸ“… 1986: Backpropagation (Rumelhart) - Revival of NNs",
            "ğŸ“… 1998: LeNet-5 (LeCun) - First successful CNN for digits",
            "ğŸ“… 2006: Deep Belief Networks (Hinton) - Deep Learning term coined",
            "ğŸ“… 2012: AlexNet - ImageNet breakthrough (15.3% error â†’ 26.2%)",
            "ğŸ“… 2014-Present: ResNet, GANs, Transformers, GPT, BERT",
            "âš¡ AKTU: Timeline questions, AlexNet significance"
        ]
    },
    {
        id: 2,
        title: "2. Probabilistic Theory of Deep Learning",
        frontText: "Statistical Foundations of Neural Networks",
        backTitle: "Probabilistic Framework",
        importance: "medium",
        backContent: [
            "ğŸ“Š NN as PROBABILISTIC MODEL:",
            "  â€¢ Output = P(y|x; Î¸) where Î¸ are parameters",
            "  â€¢ Softmax for multi-class: P(y=k) = e^zâ‚– / Î£e^zâ±¼",
            "ğŸ¯ MAXIMUM LIKELIHOOD ESTIMATION (MLE):",
            "  â€¢ Maximize P(data|Î¸) = Î áµ¢ P(yáµ¢|xáµ¢; Î¸)",
            "  â€¢ Equivalent to minimizing negative log-likelihood",
            "ğŸ“‰ CROSS-ENTROPY = Negative Log-Likelihood",
            "ğŸ”„ BAYESIAN VIEW:",
            "  â€¢ P(Î¸|data) âˆ P(data|Î¸)Â·P(Î¸)",
            "  â€¢ Regularization = Prior on weights",
            "âš¡ AKTU: Softmax derivation, MLE connection to cross-entropy"
        ]
    },
    {
        id: 3,
        title: "3. Regularization Techniques",
        frontText: "Preventing Overfitting in Deep Networks",
        backTitle: "Regularization Methods",
        importance: "high",
        backContent: [
            "âš ï¸ OVERFITTING: Model memorizes training data, fails on new data",
            "ğŸ”§ L1 REGULARIZATION (Lasso):",
            "  â€¢ Loss + Î»Î£|wáµ¢| â†’ Sparse weights (feature selection)",
            "ğŸ”§ L2 REGULARIZATION (Ridge/Weight Decay):",
            "  â€¢ Loss + Î»Î£wáµ¢Â² â†’ Smaller weights, prevents large values",
            "ğŸ² DROPOUT (Hinton 2014):",
            "  â€¢ Randomly drop neurons (p=0.5) during training",
            "  â€¢ Ensemble effect, prevents co-adaptation",
            "ğŸ“Š EARLY STOPPING: Stop when validation error increases",
            "ğŸ“ˆ DATA AUGMENTATION: Flip, rotate, crop images",
            "âš¡ AKTU Important: L1 vs L2, Dropout mechanism, when to use each"
        ]
    },
    {
        id: 4,
        title: "4. Batch Normalization",
        frontText: "Normalizing Layer Inputs for Faster Training",
        backTitle: "Batch Norm Details",
        importance: "high",
        backContent: [
            "ğŸ¯ PROBLEM: Internal Covariate Shift - distribution of inputs changes",
            "ğŸ“ BATCH NORM ALGORITHM:",
            "  1. Î¼ = (1/m)Î£xáµ¢ (batch mean)",
            "  2. ÏƒÂ² = (1/m)Î£(xáµ¢ - Î¼)Â² (batch variance)",
            "  3. xÌ‚áµ¢ = (xáµ¢ - Î¼) / âˆš(ÏƒÂ² + Îµ) (normalize)",
            "  4. yáµ¢ = Î³xÌ‚áµ¢ + Î² (scale and shift - learned params)",
            "âœ… BENEFITS:",
            "  â€¢ Faster training (higher learning rates)",
            "  â€¢ Reduces sensitivity to initialization",
            "  â€¢ Acts as regularizer (slight noise)",
            "âš¡ AKTU VERY IMPORTANT: Derive BN, explain why it works"
        ]
    },
    {
        id: 5,
        title: "5. VC Dimension & Neural Networks",
        frontText: "Measuring Model Complexity",
        backTitle: "VC Dimension Theory",
        importance: "medium",
        backContent: [
            "ğŸ“Š VC DIMENSION: Max points a classifier can shatter",
            "ğŸ” SHATTER: Correctly classify for ALL possible labelings",
            "ğŸ“ EXAMPLES:",
            "  â€¢ Linear classifier in 2D: VC = 3 (can shatter 3 points, not 4)",
            "  â€¢ Perceptron in n-D: VC = n + 1",
            "ğŸ§  NEURAL NETWORK VC DIMENSION:",
            "  â€¢ Roughly proportional to number of parameters",
            "  â€¢ VC(NN) = O(WÂ·log(W)) where W = weights",
            "âš–ï¸ GENERALIZATION BOUND:",
            "  â€¢ Test Error â‰¤ Training Error + O(âˆš(VC/n))",
            "  â€¢ More data OR lower VC â†’ Better generalization",
            "âš¡ AKTU: Define VC dimension, calculate for linear classifier"
        ]
    },
    {
        id: 6,
        title: "6. Deep vs Shallow Networks",
        frontText: "Why Go Deep?",
        backTitle: "Depth Advantages",
        importance: "high",
        backContent: [
            "ğŸ—ï¸ SHALLOW: 1-2 hidden layers | DEEP: Many hidden layers (10-1000+)",
            "âœ… ADVANTAGES OF DEPTH:",
            "  â€¢ Feature Hierarchy: Low â†’ Mid â†’ High level features",
            "  â€¢ Exponential Efficiency: Deep can compute with fewer neurons",
            "  â€¢ Compositional: Complex = composition of simpler functions",
            "ğŸ“Š EXAMPLE: Image Recognition",
            "  â€¢ Layer 1: Edges, gradients",
            "  â€¢ Layer 2: Corners, textures",
            "  â€¢ Layer 3: Object parts (eyes, wheels)",
            "  â€¢ Layer 4+: Full objects",
            "âš ï¸ CHALLENGES: Vanishing gradients, harder to train",
            "ğŸ”§ SOLUTIONS: ReLU, BatchNorm, Skip connections, proper init",
            "âš¡ AKTU Important: Compare deep vs shallow, feature hierarchy"
        ]
    },
    {
        id: 7,
        title: "7. Convolutional Neural Networks (CNNs)",
        frontText: "Specialized Networks for Spatial Data",
        backTitle: "CNN Architecture",
        importance: "high",
        backContent: [
            "ğŸ¯ DESIGNED FOR: Images, spatial data, grid-like topology",
            "ğŸ“ KEY LAYERS:",
            "  â€¢ CONVOLUTION: Filters detect features (edges, textures)",
            "    - Output size: (N-F+2P)/S + 1 where N=input, F=filter, P=padding, S=stride",
            "  â€¢ POOLING: Reduce spatial size (Max/Average pooling)",
            "  â€¢ FULLY CONNECTED: Classification at the end",
            "âœ… ADVANTAGES:",
            "  â€¢ Parameter sharing (same filter across image)",
            "  â€¢ Translation invariance (detect features anywhere)",
            "  â€¢ Local connectivity (focus on local patterns)",
            "ğŸ“Š TYPICAL ARCHITECTURE: [CONVâ†’ReLUâ†’POOL] Ã— N â†’ FC â†’ Softmax",
            "âš¡ AKTU VERY IMPORTANT: CNN architecture, convolution formula, pooling"
        ]
    },
    {
        id: 8,
        title: "8. Generative Adversarial Networks (GANs)",
        frontText: "Two Networks in Competition",
        backTitle: "GAN Architecture",
        importance: "high",
        backContent: [
            "ğŸ­ TWO NETWORKS:",
            "  â€¢ GENERATOR (G): Creates fake data from random noise z",
            "  â€¢ DISCRIMINATOR (D): Distinguishes real from fake",
            "ğŸ® MINIMAX GAME:",
            "  min_G max_D V(D,G) = E[log(D(x))] + E[log(1-D(G(z)))]",
            "ğŸ”„ TRAINING:",
            "  1. Train D to maximize: correctly classify real/fake",
            "  2. Train G to minimize: fool D (make D(G(z)) â‰ˆ 1)",
            "ğŸ“Š VARIANTS: DCGAN, WGAN, StyleGAN, CycleGAN",
            "ğŸ¨ APPLICATIONS:",
            "  â€¢ Image generation, Super-resolution",
            "  â€¢ Style transfer, Data augmentation",
            "âš¡ AKTU Important: GAN objective function, training process"
        ]
    },
    {
        id: 9,
        title: "9. Semi-Supervised Learning",
        frontText: "Learning with Limited Labels",
        backTitle: "Semi-Supervised Methods",
        importance: "medium",
        backContent: [
            "ğŸ¯ PROBLEM: Limited labeled data + abundant unlabeled data",
            "ğŸ“Š WHY: Labeling is expensive, unlabeled data is cheap",
            "ğŸ”§ TECHNIQUES:",
            "  â€¢ PSEUDO-LABELING: Train on labeled â†’ Predict unlabeled â†’ Add confident predictions",
            "  â€¢ CONSISTENCY REGULARIZATION: Same input + perturbations â†’ Same output",
            "  â€¢ CO-TRAINING: Multiple views/models agree on unlabeled data",
            "  â€¢ GRAPH-BASED: Propagate labels through similarity graph",
            "ğŸ§  DEEP LEARNING APPROACHES:",
            "  â€¢ MixMatch, FixMatch (state-of-the-art)",
            "  â€¢ Virtual Adversarial Training (VAT)",
            "âš¡ AKTU: Define semi-supervised, pseudo-labeling algorithm"
        ]
    }
];
