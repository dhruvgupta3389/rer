import { type FlashcardData } from '../components/Flashcard';

export const dwmUnit4Data: FlashcardData[] = [
    {
        id: 1,
        title: "1. Classification - Definition",
        frontText: "Predicting class labels",
        backTitle: "Classification Basics",
        importance: "high",
        backContent: [
            "ğŸ¯ DEFINITION: Assigns class labels to data objects.",
            "ğŸ“Š SUPERVISED LEARNING: Uses labeled training data.",
            "ğŸ“ STEPS:",
            "  1. Training: Build model from labeled data",
            "  2. Testing: Validate on test data",
            "  3. Application: Classify new data",
            "ğŸ“ˆ EXAMPLES: Spam detection, Disease diagnosis, Loan approval.",
            "âš¡ AKTU: Define classification with examples"
        ]
    },
    {
        id: 2,
        title: "2. Data Generalization & Relevance",
        frontText: "Feature analysis",
        backTitle: "Attribute Analysis",
        importance: "medium",
        backContent: [
            "ğŸ”¼ DATA GENERALIZATION: Roll-up data for higher abstraction.",
            "ğŸ” ATTRIBUTE RELEVANCE ANALYSIS:",
            "  â€¢ Which features matter?",
            "  â€¢ Information gain, correlation",
            "âš–ï¸ MINING CLASS COMPARISONS:",
            "  â€¢ Compare characteristics of different classes",
            "  â€¢ Contrast set mining",
            "âš¡ AKTU: Explain attribute relevance"
        ]
    },
    {
        id: 3,
        title: "3. Classification Algorithms",
        frontText: "Types of classifiers",
        backTitle: "Algorithm Types",
        importance: "high",
        backContent: [
            "ğŸ“Š STATISTICAL-BASED: Naive Bayes, Bayesian networks.",
            "ğŸ“ DISTANCE-BASED: k-Nearest Neighbor (k-NN).",
            "ğŸŒ³ DECISION TREE-BASED: ID3, C4.5, CART.",
            "ğŸ§  NEURAL NETWORKS: Perceptron, Backpropagation.",
            "ğŸ“ SVM: Support Vector Machines.",
            "âš¡ AKTU: Compare k-NN vs Decision Tree"
        ]
    },
    {
        id: 4,
        title: "4. Clustering - Introduction",
        frontText: "Grouping similar objects",
        backTitle: "Clustering Basics",
        importance: "high",
        backContent: [
            "ğŸ¯ DEFINITION: Group similar objects, no predefined labels.",
            "ğŸ“Š UNSUPERVISED LEARNING: Discovers structure.",
            "ğŸ” GOAL: High intra-cluster similarity, low inter-cluster.",
            "ğŸ“ˆ APPLICATIONS:",
            "  â€¢ Customer segmentation",
            "  â€¢ Image segmentation",
            "  â€¢ Anomaly detection",
            "âš¡ AKTU: Define clustering with applications"
        ]
    },
    {
        id: 5,
        title: "5. Similarity & Distance Measures",
        frontText: "How to measure closeness?",
        backTitle: "Distance Metrics",
        importance: "medium",
        backContent: [
            "ğŸ“ EUCLIDEAN DISTANCE: d = âˆšÎ£(xáµ¢ - yáµ¢)Â²",
            "ğŸš• MANHATTAN DISTANCE: d = Î£|xáµ¢ - yáµ¢|",
            "ğŸ”„ MINKOWSKI DISTANCE: Generalized form.",
            "ğŸ“ COSINE SIMILARITY: cos(Î¸) = (AÂ·B)/(|A||B|)",
            "ğŸ”— JACCARD COEFFICIENT: |Aâˆ©B| / |AâˆªB|",
            "âš¡ AKTU: When to use Euclidean vs Cosine?"
        ]
    },
    {
        id: 6,
        title: "6. Hierarchical Clustering",
        frontText: "Building cluster hierarchy",
        backTitle: "CURE & Chameleon",
        importance: "high",
        backContent: [
            "ğŸ”¼ AGGLOMERATIVE (Bottom-up): Merge clusters.",
            "ğŸ”½ DIVISIVE (Top-down): Split clusters.",
            "ğŸ©¹ CURE ALGORITHM:",
            "  â€¢ Uses representative points (not just centroid)",
            "  â€¢ Shrinks points toward center",
            "  â€¢ Handles non-spherical clusters",
            "ğŸ¦ CHAMELEON:",
            "  â€¢ Uses graph partitioning",
            "  â€¢ Considers relative closeness & interconnectivity",
            "âš¡ AKTU Important: Explain CURE algorithm"
        ]
    },
    {
        id: 7,
        title: "7. Density-Based Clustering",
        frontText: "DBSCAN & OPTICS",
        backTitle: "Density Methods",
        importance: "high",
        backContent: [
            "ğŸ¯ IDEA: Clusters are dense regions separated by sparse.",
            "ğŸ”µ DBSCAN:",
            "  â€¢ Core point: â‰¥ MinPts within Îµ-radius",
            "  â€¢ Border point: Within Îµ of core point",
            "  â€¢ Noise: Neither core nor border",
            "  â€¢ Finds arbitrary shapes, handles noise",
            "ğŸ”´ OPTICS: Handles varying density, ordering-based.",
            "âš¡ AKTU Very Important: Explain DBSCAN with diagram"
        ]
    },
    {
        id: 8,
        title: "8. Grid-Based & Model-Based",
        frontText: "STING, CLIQUE, Statistical",
        backTitle: "Other Methods",
        importance: "medium",
        backContent: [
            "ğŸ“Š GRID-BASED: Partition space into cells.",
            "  â€¢ STING: Statistical Info Grid",
            "  â€¢ CLIQUE: Subspace clustering",
            "  â€¢ Fast, independent of data size",
            "ğŸ“ˆ MODEL-BASED: Assume data follows distribution.",
            "  â€¢ EM Algorithm (Expectation-Maximization)",
            "  â€¢ Gaussian Mixture Models",
            "âš¡ AKTU: Compare STING vs CLIQUE"
        ]
    },
    {
        id: 9,
        title: "9. Association Rules - Basics",
        frontText: "Finding item relationships",
        backTitle: "Association Mining",
        importance: "high",
        backContent: [
            "ğŸ›’ MARKET BASKET: {Bread, Butter} â†’ {Milk}",
            "ğŸ“Š SUPPORT: P(X âˆª Y) - frequency of itemset.",
            "ğŸ“ˆ CONFIDENCE: P(Y|X) = Support(XâˆªY)/Support(X).",
            "ğŸ¯ LIFT: Confidence / P(Y) - strength of rule.",
            "âš¡ AKTU Very Important: Calculate Support & Confidence"
        ]
    },
    {
        id: 10,
        title: "10. Apriori Algorithm",
        frontText: "Mining frequent itemsets",
        backTitle: "Apriori Steps",
        importance: "high",
        backContent: [
            "ğŸ¯ APRIORI PRINCIPLE: If itemset is infrequent, all supersets are infrequent.",
            "ğŸ“‹ ALGORITHM STEPS:",
            "  1. Find frequent 1-itemsets (L1)",
            "  2. Generate candidate k-itemsets (Ck)",
            "  3. Prune using support threshold",
            "  4. Repeat until no more frequent itemsets",
            "ğŸ“Š RULE GENERATION: From frequent itemsets, generate rules.",
            "âš¡ AKTU: Apply Apriori on sample transactions"
        ]
    }
];
